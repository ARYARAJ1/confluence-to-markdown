<!DOCTYPE html>
<html>
    <head>
        <title>SDI : Data Warehouse Pipeline - 2. Orchestration</title>
        <link rel="stylesheet" href="styles/site.css" type="text/css" />
        <META http-equiv="Content-Type" content="text/html; charset=UTF-8">
    </head>

    <body class="theme-default aui-theme-default">
        <div id="page">
            <div id="main" class="aui-page-panel">
                <div id="main-header">
                    <div id="breadcrumb-section">
                        <ol id="breadcrumbs">
                            <li class="first">
                                <span><a href="index.html">SDI</a></span>
                            </li>
                                                    <li>
                                <span><a href="1205338136.html">Decisions/POCs</a></span>
                            </li>
                                                    <li>
                                <span><a href="Data-Warehouse-POC_1070989328.html">Data Warehouse POC</a></span>
                            </li>
                                                </ol>
                    </div>
                    <h1 id="title-heading" class="pagetitle">
                                                <span id="title-text">
                            SDI : Data Warehouse Pipeline - 2. Orchestration
                        </span>
                    </h1>
                </div>

                <div id="content" class="view">
                    <div class="page-metadata">
                        
        
    
        
    
        
        
            Created by <span class='author'> Sarah Fan (Deactivated)</span>, last modified by <span class='editor'> Jason Dang</span> on Feb 08, 2021
                        </div>
                    <div id="main-content" class="wiki-content group">
                    <h1 id="DataWarehousePipeline-2.Orchestration-Introduction">Introduction</h1><p>This documentation records possible options that can be used to orchestrate data warehouse pipeline. The main processes in the data warehouse pipeline are long-running SQL scripts that load data to various tables with joins, inserts, updates, etc. Therefore, the orchestration tools in consideration need to have no constraint on timeout. There will be inherent dependencies between the SQL scripts, such as ‘load to a policy table when both risk console policy extracts and the latest C360 policy extracts are available’. The chosen orchestration option needs to balance</p><ul><li><p>simplicity to build</p></li><li><p>simplicity to operate and maintain</p></li><li><p>flexibility to chain together reusable transformation components and bespoke transformation components</p></li><li><p>alignment to the existing SDI technology stack, where possible. </p></li></ul><h1 id="DataWarehousePipeline-2.Orchestration-OptionsofOrchestration">Options of Orchestration</h1><div class="table-wrap"><table data-layout="default" class="confluenceTable"><colgroup><col style="width: 64.0px;"/><col style="width: 177.0px;"/><col style="width: 164.0px;"/><col style="width: 177.0px;"/><col style="width: 178.0px;"/></colgroup><tbody><tr><th class="confluenceTh"><p /></th><th class="confluenceTh"><p><strong>Use ADF to orchestrate and run activities </strong><span class="status-macro aui-lozenge aui-lozenge-success">DECIDED</span> </p></th><th class="confluenceTh"><p><strong>Use Durable function to orchestrate and run activities</strong></p></th><th class="confluenceTh"><p><strong>Azure Databricks Workspace to orchestrate and perform ETL jobs</strong></p></th><th class="confluenceTh"><p><strong>Use Logic Apps to orchestrate and run actions</strong></p></th></tr><tr><td class="confluenceTd"><p>Description</p></td><td class="confluenceTd"><p>ADF pipeline is used to as the orchestration tool to string ADF activities and other Azure services together with built-in logic components to direct processes for different situation.</p></td><td class="confluenceTd"><p>Use a durable function as a long running execution process to codify the data warehouse pipeline with components such as an orchestrator, activities and a trigger. Activities can include a dependency checker and processes that run and monitor the outcome of SQL executions in Snowflake.</p></td><td class="confluenceTd"><p>Azure Databricks is a fully managed service that consists of two services: </p><ul><li><p>Azure Databrcks SQL Analytics <a href="https://docs.microsoft.com/en-us/azure/databricks/scenarios/what-is-azure-databricks-sqla" class="external-link" rel="nofollow">runs and visualises SQL queries on azure data lake.</a> It only integrates with Azure databases and stores, and Power BI. It’s in preview.</p></li><li><p>Azure Databricks Workspace is an analytics platform that enables multi-user collaboration based on Apache Spark. It can connect to Snowflake via <a href="https://vmia.atlassian.net/wiki/spaces/SDI/pages/398852110" rel="nofollow">Spark connector in Python, Scala, and R</a>. Databricks is a long running process that runs on a cluster in the VNET.</p></li></ul></td><td class="confluenceTd"><p> Logic Apps acts as an overarching layer that strings actions and ADF pipelines together. Unlike ADF, Logic Apps is an older service that can integrate with more Azure services than ADF. To integrate with services outside of Azure (e.g. snowflake), Logic Apps needs to trigger an Azure service that can integrate with the external service. See <a href="https://vmia.atlassian.net/wiki/spaces/SDI/pages/401605809/Azure+Logic+Apps+Use+Cases+and+Capability+Exploration" data-linked-resource-id="401605809" data-linked-resource-version="12" data-linked-resource-type="page">Azure Logic Apps Use Cases and Capability Exploration</a> <span class="inline-comment-marker" data-ref="4a87f928-fb1f-4b6c-aafe-3527a9cc8b00">for detail assessment of the </span><a href="https://vmia.atlassian.net/wiki/spaces/SDI/pages/401605809/Azure+Logic+Apps+Use+Cases+and+Capability+Exploration#Use-Case-2%3A-Use-Logic-Apps-as-More-Generic-Orchestration-Tool-for-Data-Pipelines" rel="nofollow"><span class="inline-comment-marker" data-ref="4a87f928-fb1f-4b6c-aafe-3527a9cc8b00">suitability of Logic Apps for orchestration</span></a><span class="inline-comment-marker" data-ref="4a87f928-fb1f-4b6c-aafe-3527a9cc8b00">.</span></p></td></tr><tr><td class="confluenceTd"><p>Advantages</p></td><td class="confluenceTd"><ul><li><p>Consistent user experience with Data Lake process, e.g. rerun from the failed activity, one place to monitor pipeline status.</p></li><li><p>Reuse existing knowledge on Azure services which minimises the risk of encountering unknown issues and service limitations.</p></li><li><p>Data factory is already setup with connectivity to snowflake</p></li><li><p>Easy setup via configuration with minimum code (if an activity is an Azure function).</p></li><li><p>Can easily control sequential or parallel runs in a ForEach loop.</p></li></ul></td><td class="confluenceTd"><ul><li><p>Can view durable function as a codified pipeline that contains orchestrator, activity, entity and trigger.</p></li><li><p>Can leverage <a href="https://docs.microsoft.com/en-us/azure/azure-functions/durable/quickstart-python-vscode" class="external-link" rel="nofollow">Visual Studio Code for a quick start with templates</a></p></li><li><p>Failure occurring in activities will surface to orchestration. A durable function has an easiy integration with and consumes orchestration status. There are three options:</p><ul><li><p>Using the <a href="https://vmia.atlassian.net/wiki/spaces/SDI/pages/401605809/Azure+Logic+Apps+Use+Cases+and+Capability+Exploration" rel="nofollow">custom status</a> feature to periodically update and query the status of a durable function to reflect its progress,</p></li><li><p>Using the<a href="https://docs.microsoft.com/en-us/azure/azure-functions/durable/durable-functions-http-api#get-instance-status" class="external-link" rel="nofollow"> Client Function (trigger) inside of a durable function to read the durable entity</a>,</p></li><li><p>Write progress into any databases and read back from the database in the poll HTTP endpoint.</p></li></ul></li></ul></td><td class="confluenceTd"><ul><li><p>A powerful and flexible option that can only orchestrate and run ETL processes but also heavy-lifting analytical processing.</p></li></ul></td><td class="confluenceTd"><ul><li><p>Logic Apps is considered as a <a href="https://platform.deloitte.com.au/articles/azure-durable-functions-vs-logic-apps#:~:text=Durable%20Functions%20is%20an%20extension,workflows%20through%20a%20visual%20designer." class="external-link" rel="nofollow">comparable codeless alternative of durable function for server-less and stateful workflow solution.</a> It surpasses durable function and ADF in its integration with 200+ Azure services, Visual Designer and troubleshoots.</p></li><li><p>The ForEach loop triggers actions by default in parallel and can use the concurrency setting limit to 1 to achieve sequential execution.</p></li></ul></td></tr><tr><td class="confluenceTd"><p>Disadvantages</p></td><td class="confluenceTd"><ul><li><p>In ADF Monitor, there is no default way to expose what data feed ran in each pipeline. Trigger name, Annotation and User Properties need to be leveraged to provide more metadata to make debugging experience easier.</p></li><li><p>There are some <a href="https://github.com/MicrosoftDocs/azure-docs/blob/master/includes/azure-data-factory-limits.md" class="external-link" rel="nofollow">hard limits on using ADF</a>: </p><ul><li><p>concurrent pipeline activity runs per subscription per IR region: 1000</p></li><li><p>Maximum activities per pipeline: 40</p></li><li><p>Maximum parameters per pipeline: 50</p></li><li><p>ForEach parallelism: 50</p></li><li><p>for the managed integration runtime - Data Integration Units per copy activity run: 256</p></li></ul></li></ul></td><td class="confluenceTd"><ul><li><p>It is a new service to introduce into the existing solution and therefore, may bring in uncertainty and risk of unknown issues.</p></li><li><p>A durable function is a complex service (contains several types of functions: orchestrator, activity, entity to manage state and client. ) to introduce into the solution and therefore may introduce unknown issues and limitations that require time and effort to resolve.</p></li><li><p>An orchestrator function needs to follow <a href="https://docs.microsoft.com/en-us/azure/azure-functions/durable/durable-functions-code-constraints" class="external-link" rel="nofollow">strict requirements (deterministic, i.e. an orchestrator function will be replayed multiple times, and it must produce the same result each time) on how to write the code</a>.</p></li><li><p>Activity function is <a href="https://docs.microsoft.com/en-us/azure/azure-functions/durable/durable-functions-types-features-overview" class="external-link" rel="nofollow">only guaranteed at least once execute</a> (so it will incur duplicated execution from time to time) and therefore we need to make it idempotent, which will add complexity. It can only be triggered by an orchestrator function.</p></li><li><p>Orchestrator and entity functions cannot be triggered directly using the buttons in the Azure Portal.  <a href="https://docs.microsoft.com/en-us/azure/azure-functions/durable/durable-functions-types-features-overview" class="external-link" rel="nofollow">Must run a client function</a> that starts an orchestrator or entity function as part of its implementation.  This has implications for easy operation of the platform. </p></li><li><p>A minimum complete durable function requires <a href="https://docs.microsoft.com/en-us/azure/azure-functions/durable/quickstart-python-vscode" class="external-link" rel="nofollow">an orchestrator, an activity and a client function as a trigger</a>. Python version is in preview, and only those three are available.</p></li><li><p>The entire pipeline, restart logic, logging and associated tests will need to be coded from scratch.</p></li></ul></td><td class="confluenceTd"><ul><li><p>It is a new service to introduce into the existing solution and therefore, may bring in uncertainty and risk of unknown issues.</p></li><li><p>Azure Databricks may experience some start-time and auto-scaling delay as it will add new a VM into a cluster. Azure Databricks pools can reduce the delay by having a set of VMs idle and ready to use.  <span class="inline-comment-marker" data-ref="327174ca-65df-4239-865b-c389ea1dae21">Azure doesn’t charge for Databricks Units (DBUs), but will charge for idling VMs.</span></p></li><li><p><span class="inline-comment-marker" data-ref="1f4674af-b213-482c-8320-9273a360dc5a">A workspace is </span><a href="https://vmia.atlassian.net/wiki/spaces/SDI/pages/263421967/Azure+Data+Factory+-+Build" rel="nofollow"><span class="inline-comment-marker" data-ref="1f4674af-b213-482c-8320-9273a360dc5a">limited to 1000</span></a><span class="inline-comment-marker" data-ref="1f4674af-b213-482c-8320-9273a360dc5a"> concurrent job runs or needs to handle 429 Too Many Requests response. It’s limited to 5000 job creations in a workspace incl. “run now” and “runs submit”. </span></p></li></ul></td><td class="confluenceTd"><ul><li><p>It is a new service to introduce into the existing solution and therefore, may bring in uncertainty and risk of unknown issues.</p></li><li><p><span class="inline-comment-marker" data-ref="75afd1d6-0834-4dc9-91b2-c6b41f96f6c6">Using Logic Apps requires a significant amount of changes to the existing implemented solution (if we want to keep solution consistent across Pega and Non-Pega pipeline), e.g. the hyper pipeline will be done in Logic Apps, using activity output to pass over values can no longer work if across sub-pipelines, an app can only have one trigger and therefore needs to either use a loop to process all data feeds or a different way to inform what data feeds are processed.</span></p></li><li><p><span class="inline-comment-marker" data-ref="20d749b0-cd7a-4792-a086-2254ce7dc4a3">Logic Apps doesn’t care about if any of the execution in a loop is executed successfully. It only checks if it successfully triggers an execution. Therefore, a Logic App can show successfully finishing execution while underneath actions (e.g. ADF pipelines) fail. </span>It can bring in challenges in handling dependency actions (e.g. we would need to query pipeline status after a pipeline finishes) and users need to go to two places (i.e. Logic Apps history and ADF Monitor) to have a complete picture on execution status.</p></li></ul></td></tr></tbody></table></div><p>D</p><h2 id="DataWarehousePipeline-2.Orchestration-ExplorationonEvent-drivenArchitecture">Exploration on Event-driven Architecture</h2><p>When loading data into a data warehouse, we will need to apply different methods for different tables. Some of the SQP scripts could run over a few minutes, which can cause the Azure function to timeout. To handle a long-running, event-driven architecture is considered as Azure function can execute a SQL script without waiting for it to finish running. To know whether the script ends with success, it requires Snowflake to notify us in a certain way. Therefore, this is the purpose of exploration.</p><p>We found documentation on <a href="https://docs.snowflake.com/en/sql-reference/sql/create-notification-integration.html#usage-notes" class="external-link" rel="nofollow">Snowflake notification integration</a>, which allows the integration with Azure Storage Queue. Notification integration is a feature in preview and looks promising at first glance. However, after investigating a use case in detail, it turns out the <strong>notification integration</strong> is for Snowflake to <strong>receive events</strong> from the Azure environment, rather than for Snowflake to emit events to Azure. The <a href="https://docs.snowflake.com/en/user-guide/tables-external-azure.html" class="external-link" rel="nofollow">use case</a> is about Snowflake automatically refreshes the information in an external table when a file gets added, changed or removed in Azure Storage Blob. The illustration shows the role of Snowflake in this event-driven solution.</p><div class="ap-container" id="ap-com.mxgraph.confluence.plugins.diagramly__drawio8374305917175712235">

  <div class="ap-content " id="embedded-com.mxgraph.confluence.plugins.diagramly__drawio8374305917175712235"></div>
  <script class="ap-iframe-body-script">
  (function(){
    var data = {
    "addon_key":"com.mxgraph.confluence.plugins.diagramly",
    "uniqueKey":"com.mxgraph.confluence.plugins.diagramly__drawio8374305917175712235",
    "key":"drawio",
     "moduleType":"dynamicContentMacros",      "moduleLocation":"content",         "cp":"/wiki",
            "general":"",
    "w":"",
    "h":"",
    "url":"https://ac.draw.io/connect/confluence/viewer-1-4-42.html?ceoId=460554258&diagramName=snowflake_event_driven.svg&revision=2&width=591&height=471&tbstyle=&simple=0&lbox=1&zoom=1&links=&owningPageId=460554258&displayName=snowflake_event_driven.svg&contentId=&custContentId=460357734&contentVer=2&inComment=0&aspect=&pCenter=0&hiRes=&templateUrl=&tmpBuiltIn=&xdm_e=https%3A%2F%2Fvmia.atlassian.net&xdm_c=channel-com.mxgraph.confluence.plugins.diagramly__drawio8374305917175712235&cp=%2Fwiki&xdm_deprecated_addon_key_do_not_use=com.mxgraph.confluence.plugins.diagramly&lic=active&cv=1.1162.0",
        "structuredContext": "{\"license\":{\"active\":true},\"confluence\":{\"macro\":{\"outputType\":\"html_export\",\"hash\":\"0eb5bec4-4862-470c-a097-177fb4ce86d1\",\"id\":\"0eb5bec4-4862-470c-a097-177fb4ce86d1\"},\"content\":{\"type\":\"page\",\"version\":\"16\",\"id\":\"460554258\"},\"space\":{\"key\":\"SDI\",\"id\":\"262145\"}}}",
    "contentClassifier":"content",
    "productCtx":"{\"page.id\":\"460554258\",\"macro.hash\":\"0eb5bec4-4862-470c-a097-177fb4ce86d1\",\"page.type\":\"page\",\"macro.localId\":\"\",\"simple\":\"0\",\"inComment\":\"0\",\": = | RAW | = :\":\"zoom=1|simple=0|inComment=0|pageId=460554258|custContentId=460357734|lbox=1|diagramDisplayName=snowflake_event_driven.svg|contentVer=2|revision=2|baseUrl=https://vmia.atlassian.net/wiki|diagramName=snowflake_event_driven.svg|pCenter=0|width=591|links=|tbstyle=|height=471\",\"space.id\":\"262145\",\"diagramDisplayName\":\"snowflake_event_driven.svg\",\"diagramName\":\"snowflake_event_driven.svg\",\"links\":\"\",\"tbstyle\":\"\",\"height\":\"471\",\"space.key\":\"SDI\",\"user.id\":\"60c80092624d700069d68fe9\",\"content.version\":\"16\",\"page.title\":\"Data Warehouse Pipeline - 2. Orchestration\",\"zoom\":\"1\",\"macro.body\":\"\",\"pageId\":\"460554258\",\"custContentId\":\"460357734\",\"macro.truncated\":\"false\",\"lbox\":\"1\",\"content.type\":\"page\",\"output.type\":\"html_export\",\"contentVer\":\"2\",\"page.version\":\"16\",\"revision\":\"2\",\"user.key\":\"8a7f808a79f96b90017a0d42d78807c4\",\"baseUrl\":\"https://vmia.atlassian.net/wiki\",\"pCenter\":\"0\",\"content.id\":\"460554258\",\"width\":\"591\",\"macro.id\":\"0eb5bec4-4862-470c-a097-177fb4ce86d1\"}",
    "timeZone":"UTC",
    "origin":"https://ac.draw.io",
    "hostOrigin":"https://vmia.atlassian.net",
    "sandbox":"allow-downloads allow-forms allow-modals allow-popups allow-scripts allow-same-origin allow-top-navigation-by-user-activation allow-storage-access-by-user-activation",    "pearApp":"true",        "apiMigrations": {
        "gdpr": true
    }
}
;
    if(window.AP && window.AP.subCreate) {
      window._AP.appendConnectAddon(data);
    } else {
      require(['ac/create'], function(create){
        create.appendConnectAddon(data);
      });
    }
  }());
</script>
</div>
<p>As shown in the diagram, because the Azure Storage Queue directly links to the notification integration, Snowflake is more a consumer of the events in the queue. If Snowflake is an event producer, it should directly link to something like Azure Grid to send out events. Therefore, <strong>the conclusion is that Snowflake currently cannot support the event-driven architecture as an event emitter for our use case</strong>.</p><p><strong><span style="color: rgb(255,86,48);">Note</span></strong>: have considered and explored other possibility of event-driven architecture with Snowflake. However, <a href="https://www.snowflake.com/guides/event-driven-architecture" class="external-link" rel="nofollow">Snowflake can be the sunk storage for events coming from Kafka</a>, but not an event emitter. </p><h1 id="DataWarehousePipeline-2.Orchestration-Decision">Decision</h1><p>To decide which option is most suitable, it depends on the what tool can support required activities. Based on the outcome of <a href="Data-Warehouse-Pipeline---1.-Execute-Snowflake-Query_440271104.html" data-linked-resource-id="440271104" data-linked-resource-version="32" data-linked-resource-type="page">Data Warehouse Pipeline - 1. Execute Snowflake Query</a>, <strong>ADF lookup activity triggers the execution of a stored procedure in Snowflake</strong> is the desired option for executing long-running SQL scripts in Data Warehouse Pipeline. Therefore, the comparison below is based on this option.</p><div class="table-wrap"><table data-layout="default" class="confluenceTable"><colgroup><col style="width: 155.0px;"/><col style="width: 155.0px;"/><col style="width: 155.0px;"/><col style="width: 155.0px;"/><col style="width: 140.0px;"/></colgroup><tbody><tr><th class="confluenceTh"><p><strong>Principles</strong></p></th><th class="confluenceTh"><p><strong>Use ADF to orchestrate and run activities</strong><span class="status-macro aui-lozenge aui-lozenge-success">DECIDED</span></p></th><th class="confluenceTh"><p><strong>Use Durable function to orchestrate and run activities</strong></p></th><th class="confluenceTh"><p><strong>Azure Databricks Workspace to orchestrate and perform ETL jobs</strong></p></th><th class="confluenceTh"><p><strong>Use Logic Apps to orchestrate and run actions</strong></p><p /><p /></th></tr><tr><td class="confluenceTd"><p>Simple solution is better solution.</p></td><td class="confluenceTd"><p><img class="emoticon emoticon-tick" data-emoji-id="atlassian-check_mark" data-emoji-shortname=":check_mark:" data-emoji-fallback=":check_mark:" src="images/icons/emoticons/check.png" width="16" height="16" data-emoticon-name="tick" alt="(tick)"/></p><p>Use configuration rather than code to implement the orchestration and ADF native activities. ForEach loop can handle both sequential and in parallel execution.</p></td><td class="confluenceTd"><p><img class="emoticon emoticon-cross" data-emoji-id="atlassian-cross_mark" data-emoji-shortname=":cross_mark:" data-emoji-fallback=":cross_mark:" src="images/icons/emoticons/error.png" width="16" height="16" data-emoticon-name="cross" alt="(error)"/></p><p>All the orchestration, activities, triggers will need to be codified with additional configuration for deployment.</p></td><td class="confluenceTd"><p><img class="emoticon emoticon-cross" data-emoji-id="atlassian-cross_mark" data-emoji-shortname=":cross_mark:" data-emoji-fallback=":cross_mark:" src="images/icons/emoticons/error.png" width="16" height="16" data-emoticon-name="cross" alt="(error)"/></p><p>All the orchestration and activities will need to be codified with additional configuration for deployment.</p></td><td class="confluenceTd"><p><img class="emoticon emoticon-tick" data-emoji-id="atlassian-check_mark" data-emoji-shortname=":check_mark:" data-emoji-fallback=":check_mark:" src="images/icons/emoticons/check.png" width="16" height="16" data-emoticon-name="tick" alt="(tick)"/></p><p>Use configuration rather than code to implement the orchestration and Logic Apps' native activities. Can integrate with ADF pipelines. ForEach loop by default runs in parallel but can use the setting to limit concurrency to be 1 to achieve sequential execution.</p></td></tr><tr><td class="confluenceTd"><p>Minimise the introduction of new services unless necessary.</p></td><td class="confluenceTd"><p><img class="emoticon emoticon-tick" data-emoji-id="atlassian-check_mark" data-emoji-shortname=":check_mark:" data-emoji-fallback=":check_mark:" src="images/icons/emoticons/check.png" width="16" height="16" data-emoticon-name="tick" alt="(tick)"/></p><p>No new service needed.</p></td><td class="confluenceTd"><p><img class="emoticon emoticon-cross" data-emoji-id="atlassian-cross_mark" data-emoji-shortname=":cross_mark:" data-emoji-fallback=":cross_mark:" src="images/icons/emoticons/error.png" width="16" height="16" data-emoticon-name="cross" alt="(error)"/></p><p>Durable Function is a new service to the existing solution.</p></td><td class="confluenceTd"><p><img class="emoticon emoticon-cross" data-emoji-id="atlassian-cross_mark" data-emoji-shortname=":cross_mark:" data-emoji-fallback=":cross_mark:" src="images/icons/emoticons/error.png" width="16" height="16" data-emoticon-name="cross" alt="(error)"/></p><p>Azure Databricks is a new service to the existing solution.</p></td><td class="confluenceTd"><p><img class="emoticon emoticon-cross" data-emoji-id="atlassian-cross_mark" data-emoji-shortname=":cross_mark:" data-emoji-fallback=":cross_mark:" src="images/icons/emoticons/error.png" width="16" height="16" data-emoticon-name="cross" alt="(error)"/></p><p>Logic Apps is a new service to the existing solution.</p></td></tr><tr><td class="confluenceTd"><p>It has to be easy to use for on-going maintenance and operation.</p></td><td class="confluenceTd"><p><img class="emoticon emoticon-tick" data-emoji-id="atlassian-check_mark" data-emoji-shortname=":check_mark:" data-emoji-fallback=":check_mark:" src="images/icons/emoticons/check.png" width="16" height="16" data-emoticon-name="tick" alt="(tick)"/></p><p>ADF Monitor contains all the logs including errors from the Snowflake side. Logs are also available in Log Analytics workspace.</p></td><td class="confluenceTd"><p><img class="emoticon emoticon-cross" data-emoji-id="atlassian-cross_mark" data-emoji-shortname=":cross_mark:" data-emoji-fallback=":cross_mark:" src="images/icons/emoticons/error.png" width="16" height="16" data-emoticon-name="cross" alt="(error)"/></p><p>A data engineer needs to check both ADF Monitor and Durable Function logs (i.e. App Insights). All the logs can be accessible from Log Analytics workspace.</p></td><td class="confluenceTd"><p><img class="emoticon emoticon-cross" data-emoji-id="atlassian-cross_mark" data-emoji-shortname=":cross_mark:" data-emoji-fallback=":cross_mark:" src="images/icons/emoticons/error.png" width="16" height="16" data-emoticon-name="cross" alt="(error)"/></p><p>Azure Databricks logs can be sent to a Log Analytics workspace but <a href="https://docs.microsoft.com/en-us/azure/architecture/databricks-monitoring/application-logs" class="external-link" rel="nofollow">requires to leverage some libraries and implement in code</a>.</p></td><td class="confluenceTd"><p><img class="emoticon emoticon-cross" data-emoji-id="atlassian-cross_mark" data-emoji-shortname=":cross_mark:" data-emoji-fallback=":cross_mark:" src="images/icons/emoticons/error.png" width="16" height="16" data-emoticon-name="cross" alt="(error)"/></p><p>There is no user friendly GUI to see Logic Apps status except in the Designer view using manually trigger. Logs are available in Log Analytics workspace.</p></td></tr><tr><td class="confluenceTd"><p>Let a service to do it best at and leverage the built-in optimisation.</p></td><td class="confluenceTd"><p><img class="emoticon emoticon-tick" data-emoji-id="atlassian-check_mark" data-emoji-shortname=":check_mark:" data-emoji-fallback=":check_mark:" src="images/icons/emoticons/check.png" width="16" height="16" data-emoticon-name="tick" alt="(tick)"/></p><p>ADF is built to orchestrate and run ETL jobs.</p></td><td class="confluenceTd"><p><img class="emoticon emoticon-tick" data-emoji-id="atlassian-check_mark" data-emoji-shortname=":check_mark:" data-emoji-fallback=":check_mark:" src="images/icons/emoticons/check.png" width="16" height="16" data-emoticon-name="tick" alt="(tick)"/></p><p>Durable function is built to orchestrate and execute activities with flexibility.</p></td><td class="confluenceTd"><p><img class="emoticon emoticon-cross" data-emoji-id="atlassian-cross_mark" data-emoji-shortname=":cross_mark:" data-emoji-fallback=":cross_mark:" src="images/icons/emoticons/error.png" width="16" height="16" data-emoticon-name="cross" alt="(error)"/></p><p>Azure Databricks is more built to execute SQL on Azure data storages or run analytical computation.</p></td><td class="confluenceTd"><p><img class="emoticon emoticon-tick" data-emoji-id="atlassian-check_mark" data-emoji-shortname=":check_mark:" data-emoji-fallback=":check_mark:" src="images/icons/emoticons/check.png" width="16" height="16" data-emoticon-name="tick" alt="(tick)"/></p><p>Logic Apps is built for integration and orchestration a large number of Azure services.</p></td></tr></tbody></table></div><p />
                    </div>

                                        <div class="pageSection group">
                        <div class="pageSectionHeader">
                            <h2 id="attachments" class="pageSectionTitle">Attachments:</h2>
                        </div>

                        <div class="greybox" align="left">
                                                            <img src="images/icons/bullet_blue.gif" height="8" width="8" alt=""/>
                                <a href="attachments/460554258/460619841.tmp">~drawio~5dc205aeb6e6b50c58aeca8b~snowflake_event_driven.svg.tmp</a> (application/vnd.jgraph.mxfile)
                                <br/>
                                                            <img src="images/icons/bullet_blue.gif" height="8" width="8" alt=""/>
                                <a href="attachments/460554258/460619850.tmp">~drawio~5dc205aeb6e6b50c58aeca8b~snowflake_event_driven.svg.tmp</a> (application/vnd.jgraph.mxfile)
                                <br/>
                                                            <img src="images/icons/bullet_blue.gif" height="8" width="8" alt=""/>
                                <a href="attachments/460554258/460685413.tmp">~drawio~5dc205aeb6e6b50c58aeca8b~snowflake_event_driven.svg.tmp</a> (application/vnd.jgraph.mxfile)
                                <br/>
                                                            <img src="images/icons/bullet_blue.gif" height="8" width="8" alt=""/>
                                <a href="attachments/460554258/460685422.tmp">~drawio~5dc205aeb6e6b50c58aeca8b~snowflake_event_driven.svg.tmp</a> (application/vnd.jgraph.mxfile)
                                <br/>
                                                            <img src="images/icons/bullet_blue.gif" height="8" width="8" alt=""/>
                                <a href="attachments/460554258/460292206.tmp">~drawio~5dc205aeb6e6b50c58aeca8b~snowflake_event_driven.svg.tmp</a> (application/vnd.jgraph.mxfile)
                                <br/>
                                                            <img src="images/icons/bullet_blue.gif" height="8" width="8" alt=""/>
                                <a href="attachments/460554258/460128365.tmp">~drawio~5dc205aeb6e6b50c58aeca8b~snowflake_event_driven.svg.tmp</a> (application/vnd.jgraph.mxfile)
                                <br/>
                                                            <img src="images/icons/bullet_blue.gif" height="8" width="8" alt=""/>
                                <a href="attachments/460554258/460161134.tmp">~drawio~5dc205aeb6e6b50c58aeca8b~snowflake_event_driven.svg.tmp</a> (application/vnd.jgraph.mxfile)
                                <br/>
                                                            <img src="images/icons/bullet_blue.gif" height="8" width="8" alt=""/>
                                <a href="attachments/460554258/460554312.tmp">~drawio~5dc205aeb6e6b50c58aeca8b~snowflake_event_driven.svg.tmp</a> (application/vnd.jgraph.mxfile)
                                <br/>
                                                            <img src="images/icons/bullet_blue.gif" height="8" width="8" alt=""/>
                                <a href="attachments/460554258/460456059.tmp">~drawio~5dc205aeb6e6b50c58aeca8b~snowflake_event_driven.svg.tmp</a> (application/vnd.jgraph.mxfile)
                                <br/>
                                                            <img src="images/icons/bullet_blue.gif" height="8" width="8" alt=""/>
                                <a href="attachments/460554258/460128333.tmp">~drawio~5dc205aeb6e6b50c58aeca8b~snowflake_event_driven.svg.tmp</a> (application/vnd.jgraph.mxfile)
                                <br/>
                                                            <img src="images/icons/bullet_blue.gif" height="8" width="8" alt=""/>
                                <a href="attachments/460554258/460652610.svg">snowflake_event_driven.svg</a> (application/vnd.jgraph.mxfile)
                                <br/>
                                                            <img src="images/icons/bullet_blue.gif" height="8" width="8" alt=""/>
                                <a href="attachments/460554258/460554325.png">snowflake_event_driven.svg.png</a> (image/png)
                                <br/>
                                                            <img src="images/icons/bullet_blue.gif" height="8" width="8" alt=""/>
                                <a href="attachments/460554258/460292215.tmp">~snowflake_event_driven.svg.tmp</a> (application/vnd.jgraph.mxfile)
                                <br/>
                                                            <img src="images/icons/bullet_blue.gif" height="8" width="8" alt=""/>
                                <a href="attachments/460554258/460456076.tmp">~snowflake_event_driven.svg.tmp</a> (application/vnd.jgraph.mxfile)
                                <br/>
                                                            <img src="images/icons/bullet_blue.gif" height="8" width="8" alt=""/>
                                <a href="attachments/460554258/460685431.svg">snowflake_event_driven.svg</a> (application/vnd.jgraph.mxfile)
                                <br/>
                                                            <img src="images/icons/bullet_blue.gif" height="8" width="8" alt=""/>
                                <a href="attachments/460554258/460456068.png">snowflake_event_driven.svg.png</a> (image/png)
                                <br/>
                                                    </div>
                    </div>
                    
                                                      
                </div>             </div> 
            <div id="footer" role="contentinfo">
                <section class="footer-body">
                    <p>Document generated by Confluence on Dec 15, 2021 03:34</p>
                    <div id="footer-logo"><a href="http://www.atlassian.com/">Atlassian</a></div>
                </section>
            </div>
        </div>     </body>
</html>
